# docker-compose.yml
# Define e orquestra toda a pilha de serviços com um único ficheiro.
# > docker-compose up --build --scale producer=2

version: '3.8'

networks:
  app-net:
    driver: bridge

volumes:
  postgres-data:
    driver: local

services:

  historical-load:
    build:
      context: .
      target: app-spark
    networks: [app-net]
    container_name: historical_load
    volumes:
      - ./deltalake:/app/deltalake
    environment:
      DELTALAKE_BASE_PATH: /app/deltalake
      PIPELINE_ENV: ${PIPELINE_ENV}
    command: >
      /opt/spark/bin/spark-submit 
      --packages io.delta:delta-spark_2.13:4.0.0,org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.0
      --jars /opt/spark/jars/postgresql-42.7.7.jar
      --master local[*]
      historical_pipeline/historical_load.py

  incremental-bronze-worker:
    build:
      context: .
      target: app-spark
    networks: [app-net]
    container_name: incremental_bronze_worker
    depends_on:
      historical-load: { condition: service_completed_successfully }
    volumes:
      - ./deltalake:/app/deltalake
    environment:
      DELTALAKE_BASE_PATH: /app/deltalake
      PIPELINE_ENV: ${PIPELINE_ENV}
    command: >
      /opt/spark/bin/spark-submit 
      --packages io.delta:delta-spark_2.13:4.0.0,org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.0
      --jars /opt/spark/jars/postgresql-42.7.7.jar
      --master local[*]
      historical_pipeline/incremental_bronze_worker.py

  historical-streaming:
    build:
      context: .
      target: app-spark
    networks: [app-net]
    container_name: historical_streaming
    depends_on:
      incremental-bronze-worker: { condition: service_started }
    volumes:
      - ./deltalake:/app/deltalake
    environment:
      DELTALAKE_BASE_PATH: /app/deltalake
      PIPELINE_ENV: ${PIPELINE_ENV}
    command: >
      /opt/spark/bin/spark-submit 
      --packages io.delta:delta-spark_2.13:4.0.0,org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.0
      --master local[*]
      historical_pipeline/streaming_hist_metrics.py