# docker-compose.yml
# Define e orquestra toda a pilha de serviços com um único ficheiro.
# > docker-compose up --build --scale producer=2

version: '3.8'

networks:
  app-net:
    driver: bridge

volumes:
  postgres-data:
    driver: local

services:
  # --- SERVIÇOS DE INFRAESTRUTURA ---

  zookeeper:
    image: confluentinc/cp-zookeeper:7.3.2
    container_name: zookeeper
    networks:
      - app-net
    environment: { ZOOKEEPER_CLIENT_PORT: 2181 }


  kafka:
    image: confluentinc/cp-kafka:7.3.2
    container_name: kafka
    networks:
      - app-net
    ports: ["9092:9092"]
    depends_on: [zookeeper]
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      # --- CORREÇÃO DE REDE DEFINITIVA ---
      # Usamos um único 'listener' que funciona para tudo.
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      # --- FIM DA CORREÇÃO ---
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
    healthcheck:
      test: ["CMD-SHELL", "kafka-topics --bootstrap-server localhost:9092 --list"]
      interval: 10s
      timeout: 5s
      retries: 5

  postgres:
    image: postgres:14-alpine
    container_name: postgres_db
    networks:
      - app-net
    ports: ["5432:5432"]
    volumes: ["postgres-data:/var/lib/postgresql/data"]
    environment: { POSTGRES_USER: postgres, POSTGRES_PASSWORD: 123, POSTGRES_DB: ecommerce_db }
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres -d ecommerce_db"]
      interval: 5s
      retries: 5
    restart: unless-stopped

  redis:
    image: redis:7.2-alpine
    container_name: redis
    ports: ["6379:6379"]
    networks:
      - app-net
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      retries: 5

  # --- SERVIÇOS DA APLICAÇÃO ---

  db-init:
    build:
      context: .
      target: app-python
    networks: [app-net]
    depends_on: { postgres: { condition: service_healthy } }
    environment: { DB_HOST: postgres }
    command: >
      sh -c "python db/create_db.py && python db/carga_inicial.py"

  persist-sim-consumer:
    build:
      context: .
      target: app-python
    networks: [app-net]
    depends_on:
      kafka: { condition: service_healthy }
      db-init: { condition: service_completed_successfully }
      producer: { condition: service_started }
    environment:
      DB_HOST: postgres
      KAFKA_HOST: kafka:9092
    command: ["python", "persist_sim_consumer.py"]
  
  producer:
    build:
      context: .
      target: app-python
    networks: [app-net]
    depends_on:
      kafka: { condition: service_healthy }
      db-init: { condition: service_completed_successfully }
    environment:
      DB_HOST: postgres
      KAFKA_HOST: kafka:9092
      NUM_WORKERS: 4
    command: ["python", "producer.py"]
    restart: on-failure
  
  spark-consumer:
    build:
      context: .
      target: app-spark
    networks: [app-net]
    depends_on:
      kafka: { condition: service_healthy }
      redis: { condition: service_healthy }
    environment:
      KAFKA_HOST: kafka:9092
      REDIS_HOST: redis
    command: >
      /opt/spark/bin/spark-submit
      --packages org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.0
      --conf "spark.driver.log.level=ERROR"
      --master local[*]
      spark_consumer.py

  dashboard:
    build:
      context: .
      target: app-python
    networks: [app-net]
    ports: ["8501:8501"]
    depends_on: { redis: { condition: service_healthy } }
    environment: { REDIS_HOST: redis, POSTGRES_USER: postgres, POSTGRES_PASSWORD: 123, POSTGRES_DB: ecommerce_db }
    command: ["streamlit", "run", "app.py", "--server.port=8501", "--server.address=0.0.0.0"]
